{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegodeville16/KCGB-Complex/blob/main/Download_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download images from iNaturalist using the species ID"
      ],
      "metadata": {
        "id": "hgb6BAMoUhww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SAV2gQcG2gd",
        "outputId": "5906a7b4-a62d-4b2f-83e9-5aa929d21508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download images of *Sebastes atrovirens* and metadata"
      ],
      "metadata": {
        "id": "TI9ddN7ocN5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create a folder for images\n",
        "folder_path = \"/content/atrovirens_images\"\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "def get_unique_inaturalist_images(taxon_id, max_images=250, start_page=1, end_page=10):\n",
        "    image_data = []\n",
        "    seen_specimen_ids = set()\n",
        "    for page in range(start_page, end_page + 1):\n",
        "        url = f\"https://api.inaturalist.org/v1/observations?taxon_id={taxon_id}&per_page=100&page={page}\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "\n",
        "            for result in data['results']:\n",
        "                specimen_id = result.get('id')\n",
        "                if specimen_id in seen_specimen_ids:\n",
        "                    continue  # Skip if this specimen ID has been processed\n",
        "\n",
        "                seen_specimen_ids.add(specimen_id)\n",
        "\n",
        "                if 'photos' in result:\n",
        "                    for photo in result['photos']:\n",
        "                        image_url = photo['url'].replace(\"square\", \"original\")  # Full-sized images\n",
        "                        metadata = {\n",
        "                            'image_url': image_url,\n",
        "                            'date_observed': result.get('observed_on', 'N/A'),\n",
        "                            'location': result.get('geojson', 'N/A'),\n",
        "                            'observer': result.get('user', {}).get('login', 'N/A'),\n",
        "                        }\n",
        "                        image_data.append(metadata)\n",
        "\n",
        "                        if len(image_data) >= max_images:\n",
        "                            return image_data  # Return once the desired number of unique images is reached\n",
        "\n",
        "        else:\n",
        "            print(f\"Failed to retrieve data: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "    return image_data  # Return collected images if max_images is not reached\n",
        "\n",
        "def download_images(image_data, folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Create a CSV file to store metadata with name \"caurinus_metadata.csv\"\n",
        "    csv_file = os.path.join(folder_path, \"atrovirens_metadata.csv\")\n",
        "    with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"filename\", \"image_url\", \"date_observed\", \"location\", \"observer\"])  # CSV headers\n",
        "\n",
        "        for idx, data in enumerate(image_data):\n",
        "            image_url = data['image_url']\n",
        "            filename = f'atrovirens_{idx + 1}.jpg'  # Labeling images as atrovirens_X.jpg\n",
        "\n",
        "            try:\n",
        "                response = requests.get(image_url, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    with open(f'{folder_path}/{filename}', 'wb') as handler:\n",
        "                        handler.write(response.content)\n",
        "                    print(f\"Downloaded {filename}\")\n",
        "\n",
        "                    # Save metadata in the CSV file\n",
        "                    writer.writerow([filename, data['image_url'], data['date_observed'], data['location'], data['observer']])\n",
        "                else:\n",
        "                    print(f\"Failed to download image {idx + 1}: Status code {response.status_code}\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading {image_url}: {e}\")\n",
        "\n",
        "# Example usage\n",
        "species_taxon_id = 64481  # Replace with the actual taxon ID\n",
        "max_images = 300  # Number of unique images to download\n",
        "\n",
        "# Download images from the first range\n",
        "image_data_first_range = get_unique_inaturalist_images(species_taxon_id, max_images=max_images, start_page=1, end_page=10)\n",
        "\n",
        "## Download additional images beyond the first 250\n",
        "#additional_images = get_unique_inaturalist_images(species_taxon_id, max_images=10, start_page=9, end_page=10)\n",
        "\n",
        "# Combine the datasets\n",
        "#image_data = image_data_first_range + additional_images\n",
        "\n",
        "# Download images and metadata\n",
        "download_images(image_data_first_range, folder_path)\n",
        "\n",
        "# Create a zip file of the images and the metadata CSV\n",
        "shutil.make_archive(\"/content/atrovirens_images\", 'zip', folder_path)\n",
        "\n",
        "# Provide a download link\n",
        "files.download(\"/content/atrovirens_images.zip\")"
      ],
      "metadata": {
        "id": "pBC1bWURQDyZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download images of *Sebastes chrysomelas* and metadata"
      ],
      "metadata": {
        "id": "3_D_YUg9cUcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create a folder for images\n",
        "folder_path = \"/content/chrysomelas_images\"\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "def get_unique_inaturalist_images(taxon_id, max_images=250, start_page=1, end_page=10):\n",
        "    image_data = []\n",
        "    seen_specimen_ids = set()\n",
        "    for page in range(start_page, end_page + 1):\n",
        "        url = f\"https://api.inaturalist.org/v1/observations?taxon_id={taxon_id}&per_page=100&page={page}\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "\n",
        "            for result in data['results']:\n",
        "                specimen_id = result.get('id')\n",
        "                if specimen_id in seen_specimen_ids:\n",
        "                    continue  # Skip if this specimen ID has been processed\n",
        "\n",
        "                seen_specimen_ids.add(specimen_id)\n",
        "\n",
        "                if 'photos' in result:\n",
        "                    for photo in result['photos']:\n",
        "                        image_url = photo['url'].replace(\"square\", \"original\")  # Full-sized images\n",
        "                        metadata = {\n",
        "                            'image_url': image_url,\n",
        "                            'date_observed': result.get('observed_on', 'N/A'),\n",
        "                            'location': result.get('geojson', 'N/A'),\n",
        "                            'observer': result.get('user', {}).get('login', 'N/A'),\n",
        "                        }\n",
        "                        image_data.append(metadata)\n",
        "\n",
        "                        if len(image_data) >= max_images:\n",
        "                            return image_data  # Return once the desired number of unique images is reached\n",
        "\n",
        "        else:\n",
        "            print(f\"Failed to retrieve data: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "    return image_data  # Return collected images if max_images is not reached\n",
        "\n",
        "def download_images(image_data, folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Create a CSV file to store metadata with name \"caurinus_metadata.csv\"\n",
        "    csv_file = os.path.join(folder_path, \"chrysomelas_metadata.csv\")\n",
        "    with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"filename\", \"image_url\", \"date_observed\", \"location\", \"observer\"])  # CSV headers\n",
        "\n",
        "        for idx, data in enumerate(image_data):\n",
        "            image_url = data['image_url']\n",
        "            filename = f'chrysomelas_{idx + 1}.jpg'  # Labeling images as chrysomelas_X.jpg\n",
        "\n",
        "            try:\n",
        "                response = requests.get(image_url, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    with open(f'{folder_path}/{filename}', 'wb') as handler:\n",
        "                        handler.write(response.content)\n",
        "                    print(f\"Downloaded {filename}\")\n",
        "\n",
        "                    # Save metadata in the CSV file\n",
        "                    writer.writerow([filename, data['image_url'], data['date_observed'], data['location'], data['observer']])\n",
        "                else:\n",
        "                    print(f\"Failed to download image {idx + 1}: Status code {response.status_code}\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading {image_url}: {e}\")\n",
        "\n",
        "# Example usage\n",
        "species_taxon_id = 64449  # Replace with the actual taxon ID\n",
        "max_images = 300  # Number of unique images to download\n",
        "\n",
        "# Download images from the first range\n",
        "image_data_first_range = get_unique_inaturalist_images(species_taxon_id, max_images=max_images, start_page=1, end_page=10)\n",
        "\n",
        "## Download additional images beyond the first 250\n",
        "additional_images = get_unique_inaturalist_images(species_taxon_id, max_images=10, start_page=9, end_page=10)\n",
        "\n",
        "# Combine the datasets\n",
        "image_data = image_data_first_range + additional_images\n",
        "\n",
        "# Download images and metadata\n",
        "download_images(image_data, folder_path)\n",
        "\n",
        "# Create a zip file of the images and the metadata CSV\n",
        "shutil.make_archive(\"/content/chrysomelas_images\", 'zip', folder_path)\n",
        "\n",
        "# Provide a download link\n",
        "files.download(\"/content/chrysomelas_images.zip\")"
      ],
      "metadata": {
        "id": "qP5-SXv_gUPk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download images of *Sebastes carnatus* and metadata"
      ],
      "metadata": {
        "id": "eineK9SwcYw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create a folder for images\n",
        "folder_path = \"/content/carnatus_images\"\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "def get_unique_inaturalist_images(taxon_id, max_images=250, start_page=1, end_page=10):\n",
        "    image_data = []\n",
        "    seen_specimen_ids = set()\n",
        "    for page in range(start_page, end_page + 1):\n",
        "        url = f\"https://api.inaturalist.org/v1/observations?taxon_id={taxon_id}&per_page=100&page={page}\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "\n",
        "            for result in data['results']:\n",
        "                specimen_id = result.get('id')\n",
        "                if specimen_id in seen_specimen_ids:\n",
        "                    continue  # Skip if this specimen ID has been processed\n",
        "\n",
        "                seen_specimen_ids.add(specimen_id)\n",
        "\n",
        "                if 'photos' in result:\n",
        "                    for photo in result['photos']:\n",
        "                        image_url = photo['url'].replace(\"square\", \"original\")  # Full-sized images\n",
        "                        metadata = {\n",
        "                            'image_url': image_url,\n",
        "                            'date_observed': result.get('observed_on', 'N/A'),\n",
        "                            'location': result.get('geojson', 'N/A'),\n",
        "                            'observer': result.get('user', {}).get('login', 'N/A'),\n",
        "                        }\n",
        "                        image_data.append(metadata)\n",
        "\n",
        "                        if len(image_data) >= max_images:\n",
        "                            return image_data  # Return once the desired number of unique images is reached\n",
        "\n",
        "        else:\n",
        "            print(f\"Failed to retrieve data: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "    return image_data  # Return collected images if max_images is not reached\n",
        "\n",
        "def download_images(image_data, folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Create a CSV file to store metadata with name \"caurinus_metadata.csv\"\n",
        "    csv_file = os.path.join(folder_path, \"carnatus_metadata.csv\")\n",
        "    with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"filename\", \"image_url\", \"date_observed\", \"location\", \"observer\"])  # CSV headers\n",
        "\n",
        "        for idx, data in enumerate(image_data):\n",
        "            image_url = data['image_url']\n",
        "            filename = f'carnatus_{idx + 1}.jpg'  # Labeling images as chrysomelas_X.jpg\n",
        "\n",
        "            try:\n",
        "                response = requests.get(image_url, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    with open(f'{folder_path}/{filename}', 'wb') as handler:\n",
        "                        handler.write(response.content)\n",
        "                    print(f\"Downloaded {filename}\")\n",
        "\n",
        "                    # Save metadata in the CSV file\n",
        "                    writer.writerow([filename, data['image_url'], data['date_observed'], data['location'], data['observer']])\n",
        "                else:\n",
        "                    print(f\"Failed to download image {idx + 1}: Status code {response.status_code}\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading {image_url}: {e}\")\n",
        "\n",
        "# Example usage\n",
        "species_taxon_id = 64455  # Replace with the actual taxon ID\n",
        "max_images = 300  # Number of unique images to download\n",
        "\n",
        "# Download images from the first range\n",
        "image_data_first_range = get_unique_inaturalist_images(species_taxon_id, max_images=max_images, start_page=1, end_page=10)\n",
        "\n",
        "## Download additional images beyond the first 250\n",
        "additional_images = get_unique_inaturalist_images(species_taxon_id, max_images=10, start_page=9, end_page=10)\n",
        "\n",
        "# Combine the datasets\n",
        "image_data = image_data_first_range + additional_images\n",
        "\n",
        "# Download images and metadata\n",
        "download_images(image_data, folder_path)\n",
        "\n",
        "# Create a zip file of the images and the metadata CSV\n",
        "shutil.make_archive(\"/content/carnatus_images\", 'zip', folder_path)\n",
        "\n",
        "# Provide a download link\n",
        "files.download(\"/content/carnatus_images.zip\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jpnljDaOjULS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download images of *Sebastes caurinus* and metadata"
      ],
      "metadata": {
        "id": "O_ek1rNpccWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create a folder for images\n",
        "folder_path = \"/content/caurinus_images\"\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "def get_unique_inaturalist_images(taxon_id, max_images=250, start_page=1, end_page=10):\n",
        "    image_data = []\n",
        "    seen_specimen_ids = set()\n",
        "    for page in range(start_page, end_page + 1):\n",
        "        url = f\"https://api.inaturalist.org/v1/observations?taxon_id={taxon_id}&per_page=100&page={page}\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "\n",
        "            for result in data['results']:\n",
        "                specimen_id = result.get('id')\n",
        "                if specimen_id in seen_specimen_ids:\n",
        "                    continue  # Skip if this specimen ID has been processed\n",
        "\n",
        "                seen_specimen_ids.add(specimen_id)\n",
        "\n",
        "                if 'photos' in result:\n",
        "                    for photo in result['photos']:\n",
        "                        image_url = photo['url'].replace(\"square\", \"original\")  # Full-sized images\n",
        "                        metadata = {\n",
        "                            'image_url': image_url,\n",
        "                            'date_observed': result.get('observed_on', 'N/A'),\n",
        "                            'location': result.get('geojson', 'N/A'),\n",
        "                            'observer': result.get('user', {}).get('login', 'N/A'),\n",
        "                        }\n",
        "                        image_data.append(metadata)\n",
        "\n",
        "                        if len(image_data) >= max_images:\n",
        "                            return image_data  # Return once the desired number of unique images is reached\n",
        "\n",
        "        else:\n",
        "            print(f\"Failed to retrieve data: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "    return image_data  # Return collected images if max_images is not reached\n",
        "\n",
        "def download_images(image_data, folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Create a CSV file to store metadata with name \"caurinus_metadata.csv\"\n",
        "    csv_file = os.path.join(folder_path, \"caurinus_metadata.csv\")\n",
        "    with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"filename\", \"image_url\", \"date_observed\", \"location\", \"observer\"])  # CSV headers\n",
        "\n",
        "        for idx, data in enumerate(image_data):\n",
        "            image_url = data['image_url']\n",
        "            filename = f'caurinus_{idx + 1}.jpg'  # Labeling images as chrysomelas_X.jpg\n",
        "\n",
        "            try:\n",
        "                response = requests.get(image_url, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    with open(f'{folder_path}/{filename}', 'wb') as handler:\n",
        "                        handler.write(response.content)\n",
        "                    print(f\"Downloaded {filename}\")\n",
        "\n",
        "                    # Save metadata in the CSV file\n",
        "                    writer.writerow([filename, data['image_url'], data['date_observed'], data['location'], data['observer']])\n",
        "                else:\n",
        "                    print(f\"Failed to download image {idx + 1}: Status code {response.status_code}\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading {image_url}: {e}\")\n",
        "\n",
        "# Example usage\n",
        "species_taxon_id = 68597  # Replace with the actual taxon ID\n",
        "max_images = 400  # Number of unique images to download\n",
        "\n",
        "# Download images from the first range\n",
        "image_data_first_range = get_unique_inaturalist_images(species_taxon_id, max_images=max_images, start_page=1, end_page=3)\n",
        "\n",
        "## Download additional images beyond the first 250\n",
        "additional_images = get_unique_inaturalist_images(species_taxon_id, max_images=10, start_page=8, end_page=10)\n",
        "\n",
        "# Combine the datasets\n",
        "image_data = image_data_first_range + additional_images\n",
        "\n",
        "# Download images and metadata\n",
        "download_images(image_data, folder_path)\n",
        "\n",
        "# Create a zip file of the images and the metadata CSV\n",
        "shutil.make_archive(\"/content/caurinus_images\", 'zip', folder_path)\n",
        "\n",
        "# Provide a download link\n",
        "files.download(\"/content/caurinus_images.zip\")"
      ],
      "metadata": {
        "id": "foisG6X3rV70",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}